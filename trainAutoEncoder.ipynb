{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aym-workstation/anaconda3/envs/py3.7/lib/python3.7/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/home/aym-workstation/anaconda3/envs/py3.7/lib/python3.7/site-packages/sklearn/utils/extmath.py:1020: RuntimeWarning: invalid value encountered in subtract\n",
      "  new_unnormalized_variance -= correction ** 2 / new_sample_count\n",
      "/home/aym-workstation/anaconda3/envs/py3.7/lib/python3.7/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('./data/pre-processed-raw.csv')\n",
    "# Drop unnecessary columns\n",
    "df_clean = df.drop(columns=['date', 'credit_card', 'city', 'state', 'zipcode', 'credit_card_limit', 'transaction_dollar_amount', 'Long', 'Lat'])\n",
    "\n",
    "# Check if the dataframe has any NaN or infinite values\n",
    "if df_clean.isnull().values.any():\n",
    "    df_clean.fillna(df_clean.mean(), inplace=True)\n",
    "\n",
    "if np.isinf(df_clean.values).any():\n",
    "    df_clean = df_clean.replace([np.inf, -np.inf], np.nan)\n",
    "    df_clean.fillna(df_clean.mean(), inplace=True)\n",
    "\n",
    "df_array = np.array(df_clean)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(df_array)\n",
    "df_scaled = scaler.transform(df_array)\n",
    "df_scaled += 1e-10\n",
    "\n",
    "# Check for any remaining NaN values\n",
    "if np.isnan(df_scaled).any():\n",
    "    df_scaled = np.nan_to_num(df_scaled)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-18 22:41:27.550841: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-18 22:41:30.362960: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-18 22:41:30.364320: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "/home/aym-workstation/anaconda3/envs/py3.7/lib/python3.7/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7365/7365 [==============================] - 37s 5ms/step - loss: 0.5357 - val_loss: 0.5248\n",
      "Epoch 2/100\n",
      "7365/7365 [==============================] - 30s 4ms/step - loss: 0.4698 - val_loss: 0.5087\n",
      "Epoch 3/100\n",
      "7365/7365 [==============================] - 31s 4ms/step - loss: 0.4599 - val_loss: 0.5002\n",
      "Epoch 4/100\n",
      "7365/7365 [==============================] - 35s 5ms/step - loss: 0.4557 - val_loss: 0.4975\n",
      "Epoch 5/100\n",
      "7365/7365 [==============================] - 31s 4ms/step - loss: 0.4536 - val_loss: 0.4955\n",
      "Epoch 6/100\n",
      "7365/7365 [==============================] - 39s 5ms/step - loss: 0.4522 - val_loss: 0.4950\n",
      "Epoch 7/100\n",
      "7365/7365 [==============================] - 31s 4ms/step - loss: 0.4512 - val_loss: 0.4916\n",
      "Epoch 8/100\n",
      "7365/7365 [==============================] - 41s 6ms/step - loss: 0.4505 - val_loss: 0.4936\n",
      "Epoch 9/100\n",
      "7365/7365 [==============================] - 36s 5ms/step - loss: 0.4500 - val_loss: 0.4908\n",
      "Epoch 10/100\n",
      "7365/7365 [==============================] - 36s 5ms/step - loss: 0.4496 - val_loss: 0.4879\n",
      "Epoch 11/100\n",
      "7365/7365 [==============================] - 38s 5ms/step - loss: 0.4493 - val_loss: 0.4873\n",
      "Epoch 12/100\n",
      "7365/7365 [==============================] - 37s 5ms/step - loss: 0.4491 - val_loss: 0.4892\n",
      "Epoch 13/100\n",
      "7365/7365 [==============================] - 33s 4ms/step - loss: 0.4489 - val_loss: 0.4908\n",
      "Epoch 14/100\n",
      "7365/7365 [==============================] - 36s 5ms/step - loss: 0.4486 - val_loss: 0.4896\n",
      "Epoch 15/100\n",
      "7365/7365 [==============================] - 36s 5ms/step - loss: 0.4485 - val_loss: 0.4883\n",
      "Epoch 16/100\n",
      "7365/7365 [==============================] - 32s 4ms/step - loss: 0.4484 - val_loss: 0.4848\n",
      "Epoch 17/100\n",
      "7365/7365 [==============================] - 35s 5ms/step - loss: 0.4482 - val_loss: 0.4876\n",
      "Epoch 18/100\n",
      "7365/7365 [==============================] - 30s 4ms/step - loss: 0.4481 - val_loss: 0.4917\n",
      "Epoch 19/100\n",
      "7365/7365 [==============================] - 33s 5ms/step - loss: 0.4481 - val_loss: 0.4893\n",
      "Epoch 20/100\n",
      "7365/7365 [==============================] - 35s 5ms/step - loss: 0.4480 - val_loss: 0.4867\n",
      "Epoch 21/100\n",
      "7365/7365 [==============================] - 30s 4ms/step - loss: 0.4479 - val_loss: 0.4835\n",
      "Epoch 22/100\n",
      "7365/7365 [==============================] - 32s 4ms/step - loss: 0.4479 - val_loss: 0.4896\n",
      "Epoch 23/100\n",
      "7365/7365 [==============================] - 36s 5ms/step - loss: 0.4479 - val_loss: 0.4877\n",
      "Epoch 24/100\n",
      "7365/7365 [==============================] - 32s 4ms/step - loss: 0.4478 - val_loss: 0.4897\n",
      "Epoch 25/100\n",
      "7365/7365 [==============================] - 30s 4ms/step - loss: 0.4477 - val_loss: 0.4856\n",
      "Epoch 26/100\n",
      "7365/7365 [==============================] - 34s 5ms/step - loss: 0.4477 - val_loss: 0.4871\n",
      "Epoch 27/100\n",
      "7365/7365 [==============================] - 33s 4ms/step - loss: 0.4477 - val_loss: 0.4884\n",
      "Epoch 28/100\n",
      "7365/7365 [==============================] - 32s 4ms/step - loss: 0.4476 - val_loss: 0.4857\n",
      "Epoch 29/100\n",
      "7365/7365 [==============================] - 33s 4ms/step - loss: 0.4477 - val_loss: 0.4901\n",
      "Epoch 30/100\n",
      "7365/7365 [==============================] - 38s 5ms/step - loss: 0.4476 - val_loss: 0.4885\n",
      "Epoch 31/100\n",
      "7365/7365 [==============================] - 32s 4ms/step - loss: 0.4477 - val_loss: 0.4876\n",
      "Epoch 32/100\n",
      "7365/7365 [==============================] - 31s 4ms/step - loss: 0.4476 - val_loss: 0.4899\n",
      "Epoch 33/100\n",
      "7365/7365 [==============================] - 30s 4ms/step - loss: 0.4475 - val_loss: 0.4868\n",
      "Epoch 34/100\n",
      "7365/7365 [==============================] - 33s 4ms/step - loss: 0.4476 - val_loss: 0.4853\n",
      "Epoch 35/100\n",
      "7365/7365 [==============================] - 31s 4ms/step - loss: 0.4475 - val_loss: 0.4849\n",
      "Epoch 36/100\n",
      "7365/7365 [==============================] - 36s 5ms/step - loss: 0.4476 - val_loss: 0.4864\n",
      "Epoch 37/100\n",
      "7365/7365 [==============================] - 28s 4ms/step - loss: 0.4475 - val_loss: 0.4861\n",
      "Epoch 38/100\n",
      "7365/7365 [==============================] - 29s 4ms/step - loss: 0.4475 - val_loss: 0.4864\n",
      "Epoch 39/100\n",
      "7365/7365 [==============================] - 32s 4ms/step - loss: 0.4474 - val_loss: 0.4829\n",
      "Epoch 40/100\n",
      "7365/7365 [==============================] - 38s 5ms/step - loss: 0.4474 - val_loss: 0.4908\n",
      "Epoch 41/100\n",
      "7365/7365 [==============================] - 44s 6ms/step - loss: 0.4474 - val_loss: 0.4851\n",
      "Epoch 42/100\n",
      "7365/7365 [==============================] - 33s 4ms/step - loss: 0.4474 - val_loss: 0.4867\n",
      "Epoch 43/100\n",
      "7365/7365 [==============================] - 32s 4ms/step - loss: 0.4474 - val_loss: 0.4866\n",
      "Epoch 44/100\n",
      "7365/7365 [==============================] - 47s 6ms/step - loss: 0.4474 - val_loss: 0.4888\n",
      "Epoch 45/100\n",
      "7365/7365 [==============================] - 45s 6ms/step - loss: 0.4474 - val_loss: 0.4887\n",
      "Epoch 46/100\n",
      "7365/7365 [==============================] - 36s 5ms/step - loss: 0.4474 - val_loss: 0.4839\n",
      "Epoch 47/100\n",
      "7365/7365 [==============================] - 35s 5ms/step - loss: 0.4474 - val_loss: 0.4854\n",
      "Epoch 48/100\n",
      "7365/7365 [==============================] - 37s 5ms/step - loss: 0.4474 - val_loss: 0.4844\n",
      "Epoch 49/100\n",
      "7365/7365 [==============================] - 35s 5ms/step - loss: 0.4473 - val_loss: 0.4891\n",
      "Epoch 50/100\n",
      "7365/7365 [==============================] - 36s 5ms/step - loss: 0.4474 - val_loss: 0.4866\n",
      "Epoch 51/100\n",
      "7365/7365 [==============================] - 34s 5ms/step - loss: 0.4473 - val_loss: 0.4856\n",
      "Epoch 52/100\n",
      "7365/7365 [==============================] - 39s 5ms/step - loss: 0.4475 - val_loss: 0.4864\n",
      "Epoch 53/100\n",
      "7365/7365 [==============================] - 35s 5ms/step - loss: 0.4475 - val_loss: 0.4854\n",
      "Epoch 54/100\n",
      "7365/7365 [==============================] - 34s 5ms/step - loss: 0.4473 - val_loss: 0.4876\n",
      "Epoch 55/100\n",
      "7365/7365 [==============================] - 37s 5ms/step - loss: 0.4473 - val_loss: 0.4836\n",
      "Epoch 56/100\n",
      "7365/7365 [==============================] - 35s 5ms/step - loss: 0.4473 - val_loss: 0.4892\n",
      "Epoch 57/100\n",
      "7365/7365 [==============================] - 37s 5ms/step - loss: 0.4473 - val_loss: 0.4836\n",
      "Epoch 58/100\n",
      "7365/7365 [==============================] - 35s 5ms/step - loss: 0.4475 - val_loss: 0.4892\n",
      "Epoch 59/100\n",
      "7365/7365 [==============================] - 32s 4ms/step - loss: 0.4474 - val_loss: 0.4854\n",
      "Epoch 60/100\n",
      "7365/7365 [==============================] - 40s 5ms/step - loss: 0.4474 - val_loss: 0.4860\n",
      "Epoch 61/100\n",
      "7365/7365 [==============================] - 35s 5ms/step - loss: 0.4474 - val_loss: 0.4854\n",
      "Epoch 62/100\n",
      "7365/7365 [==============================] - 35s 5ms/step - loss: 0.4474 - val_loss: 0.4885\n",
      "Epoch 63/100\n",
      "7365/7365 [==============================] - 35s 5ms/step - loss: 0.4473 - val_loss: 0.4862\n",
      "Epoch 64/100\n",
      "7365/7365 [==============================] - 36s 5ms/step - loss: 0.4473 - val_loss: 0.4849\n",
      "Epoch 65/100\n",
      "7365/7365 [==============================] - 35s 5ms/step - loss: 0.4472 - val_loss: 0.4844\n",
      "Epoch 66/100\n",
      "7365/7365 [==============================] - 34s 5ms/step - loss: 0.4473 - val_loss: 0.4877\n",
      "Epoch 67/100\n",
      "7365/7365 [==============================] - 36s 5ms/step - loss: 0.4473 - val_loss: 0.4893\n",
      "Epoch 68/100\n",
      "7365/7365 [==============================] - 33s 5ms/step - loss: 0.4473 - val_loss: 0.4829\n",
      "Epoch 69/100\n",
      "7365/7365 [==============================] - 34s 5ms/step - loss: 0.4474 - val_loss: 0.4921\n",
      "Epoch 70/100\n",
      "7365/7365 [==============================] - 35s 5ms/step - loss: 0.4473 - val_loss: 0.4881\n",
      "Epoch 71/100\n",
      "7365/7365 [==============================] - 37s 5ms/step - loss: 0.4472 - val_loss: 0.4873\n",
      "Epoch 72/100\n",
      "7365/7365 [==============================] - 39s 5ms/step - loss: 0.4473 - val_loss: 0.4867\n",
      "Epoch 73/100\n",
      "7365/7365 [==============================] - 34s 5ms/step - loss: 0.4473 - val_loss: 0.4866\n",
      "Epoch 74/100\n",
      "7365/7365 [==============================] - 35s 5ms/step - loss: 0.4473 - val_loss: 0.4873\n",
      "Epoch 75/100\n",
      "7365/7365 [==============================] - 36s 5ms/step - loss: 0.4473 - val_loss: 0.4841\n",
      "Epoch 76/100\n",
      "7365/7365 [==============================] - 33s 4ms/step - loss: 0.4473 - val_loss: 0.4884\n",
      "Epoch 77/100\n",
      "7365/7365 [==============================] - 34s 5ms/step - loss: 0.4474 - val_loss: 0.4900\n",
      "Epoch 78/100\n",
      "7365/7365 [==============================] - 34s 5ms/step - loss: 0.4474 - val_loss: 0.4871\n",
      "Epoch 79/100\n",
      "7365/7365 [==============================] - 36s 5ms/step - loss: 0.4473 - val_loss: 0.4900\n",
      "Epoch 80/100\n",
      "7365/7365 [==============================] - 33s 4ms/step - loss: 0.4474 - val_loss: 0.4848\n",
      "Epoch 81/100\n",
      "7365/7365 [==============================] - 36s 5ms/step - loss: 0.4473 - val_loss: 0.4847\n",
      "Epoch 82/100\n",
      "7365/7365 [==============================] - 35s 5ms/step - loss: 0.4473 - val_loss: 0.4872\n",
      "Epoch 83/100\n",
      "7365/7365 [==============================] - 33s 4ms/step - loss: 0.4473 - val_loss: 0.4849\n",
      "Epoch 84/100\n",
      "7365/7365 [==============================] - 35s 5ms/step - loss: 0.4473 - val_loss: 0.4903\n",
      "Epoch 85/100\n",
      "7365/7365 [==============================] - 35s 5ms/step - loss: 0.4473 - val_loss: 0.4840\n",
      "Epoch 86/100\n",
      "7365/7365 [==============================] - 36s 5ms/step - loss: 0.4474 - val_loss: 0.4902\n",
      "Epoch 87/100\n",
      "7365/7365 [==============================] - 35s 5ms/step - loss: 0.4473 - val_loss: 0.4877\n",
      "Epoch 88/100\n",
      "7365/7365 [==============================] - 35s 5ms/step - loss: 0.4473 - val_loss: 0.4896\n",
      "Epoch 89/100\n",
      "7365/7365 [==============================] - 32s 4ms/step - loss: 0.4472 - val_loss: 0.4855\n",
      "Epoch 90/100\n",
      "7365/7365 [==============================] - 34s 5ms/step - loss: 0.4474 - val_loss: 0.4856\n",
      "Epoch 91/100\n",
      "7365/7365 [==============================] - 32s 4ms/step - loss: 0.4473 - val_loss: 0.4853\n",
      "Epoch 92/100\n",
      "7365/7365 [==============================] - 35s 5ms/step - loss: 0.4473 - val_loss: 0.4883\n",
      "Epoch 93/100\n",
      "7365/7365 [==============================] - 35s 5ms/step - loss: 0.4473 - val_loss: 0.4872\n",
      "Epoch 94/100\n",
      "7365/7365 [==============================] - 32s 4ms/step - loss: 0.4474 - val_loss: 0.4847\n",
      "Epoch 95/100\n",
      "7365/7365 [==============================] - 32s 4ms/step - loss: 0.4473 - val_loss: 0.4829\n",
      "Epoch 96/100\n",
      "7365/7365 [==============================] - 36s 5ms/step - loss: 0.4473 - val_loss: 0.4849\n",
      "Epoch 97/100\n",
      "7365/7365 [==============================] - 30s 4ms/step - loss: 0.4472 - val_loss: 0.4850\n",
      "Epoch 98/100\n",
      "7365/7365 [==============================] - 33s 5ms/step - loss: 0.4473 - val_loss: 0.4882\n",
      "Epoch 99/100\n",
      "7365/7365 [==============================] - 39s 5ms/step - loss: 0.4473 - val_loss: 0.4880\n",
      "Epoch 100/100\n",
      "7365/7365 [==============================] - 36s 5ms/step - loss: 0.4473 - val_loss: 0.4864\n",
      "9206/9206 [==============================] - 31s 3ms/step\n",
      "             credit_card     city state  zipcode  credit_card_limit  \\\n",
      "0       1280981422329509   Dallas    PA    18612             6000.0   \n",
      "38      1280981422329509   Dallas    PA    18612             6000.0   \n",
      "71      1280981422329509   Dallas    PA    18612             6000.0   \n",
      "100     1280981422329509   Dallas    PA    18612             6000.0   \n",
      "121     1280981422329509   Dallas    PA    18612             6000.0   \n",
      "...                  ...      ...   ...      ...                ...   \n",
      "294357  1409322756311484  Houston    PA    15342            15000.0   \n",
      "294365  1409322756311484  Houston    PA    15342            15000.0   \n",
      "294399  1409322756311484  Houston    PA    15342            15000.0   \n",
      "294533  1409322756311484  Houston    PA    15342            15000.0   \n",
      "294563  1409322756311484  Houston    PA    15342            15000.0   \n",
      "\n",
      "                       date  transaction_dollar_amount       Long        Lat  \\\n",
      "0       2015-08-05 00:59:19                      11.94 -75.964527  41.353578   \n",
      "38      2015-09-26 00:25:53                     860.42 -76.038250  41.382482   \n",
      "71      2015-09-28 01:28:54                      25.36 -75.952087  41.417648   \n",
      "100     2015-09-28 03:01:53                      50.71 -75.963789  41.260313   \n",
      "121     2015-08-03 01:04:02                      80.94 -76.059121  41.366978   \n",
      "...                     ...                        ...        ...        ...   \n",
      "294357  2015-08-18 00:07:20                      87.07 -80.175989  40.316105   \n",
      "294365  2015-09-14 02:11:18                      54.80 -80.169110  40.247454   \n",
      "294399  2015-08-04 01:44:36                      82.13 -80.280289  40.289458   \n",
      "294533  2015-10-26 01:49:00                      73.60 -80.201515  40.300957   \n",
      "294563  2015-08-04 01:54:00                     114.63 -80.215436  40.181392   \n",
      "\n",
      "        transaction_hour  transaction_day_of_week  \\\n",
      "0                      0                        2   \n",
      "38                     0                        5   \n",
      "71                     1                        0   \n",
      "100                    3                        0   \n",
      "121                    1                        0   \n",
      "...                  ...                      ...   \n",
      "294357                 0                        1   \n",
      "294365                 2                        0   \n",
      "294399                 1                        1   \n",
      "294533                 1                        0   \n",
      "294563                 1                        1   \n",
      "\n",
      "        transaction_amount_relative_to_limit  transaction_frequency  \\\n",
      "0                                   0.001990                     22   \n",
      "38                                  0.143403                    211   \n",
      "71                                  0.004227                    217   \n",
      "100                                 0.008452                    218   \n",
      "121                                 0.013490                     14   \n",
      "...                                      ...                    ...   \n",
      "294357                              0.005805                     92   \n",
      "294365                              0.003653                    211   \n",
      "294399                              0.005475                     21   \n",
      "294533                              0.004907                    403   \n",
      "294563                              0.007642                     22   \n",
      "\n",
      "        avg_transaction_amount_24h  distance_from_common_location  anomaly  \n",
      "0                        75.664091                       1.191098        1  \n",
      "38                       54.145735                       7.035974        1  \n",
      "71                       53.257650                       8.319844        1  \n",
      "100                      53.245963                       9.331849        1  \n",
      "121                     103.902143                       7.774032        1  \n",
      "...                            ...                            ...      ...  \n",
      "294357                   88.493696                       9.162940        1  \n",
      "294365                   81.078341                       4.243612        1  \n",
      "294399                  103.170000                       7.616538        1  \n",
      "294533                   78.055856                       6.898452        1  \n",
      "294563                  103.690909                       6.526768        1  \n",
      "\n",
      "[14730 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the dimension of our autoencoder\n",
    "input_dim = df_scaled.shape[1]\n",
    "encoding_dim = int(input_dim / 2)\n",
    "\n",
    "# Define the autoencoder layers\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\")(input_layer)\n",
    "decoder = Dense(input_dim, activation=\"relu\")(encoder)\n",
    "\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer=Adam(lr=0.001), loss=\"mean_squared_error\")\n",
    "\n",
    "# Train the autoencoder\n",
    "history = autoencoder.fit(df_scaled, df_scaled, epochs=100, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "\n",
    "# Use the autoencoder to reconstruct the input data\n",
    "reconstructed_data = autoencoder.predict(df_scaled)\n",
    "\n",
    "# Calculate the mean squared error for each instance\n",
    "mse = np.mean(np.power(df_scaled - reconstructed_data, 2), axis=1)\n",
    "\n",
    "# Define a threshold\n",
    "mse_threshold = np.quantile(mse, 0.95)  # consider as anomalies the 5% of the instances with highest mse\n",
    "\n",
    "# Add the anomaly prediction (anomaly=1, normal=0) to the dataframe\n",
    "df['anomaly'] = [1 if error > mse_threshold else 0 for error in mse]\n",
    "\n",
    "# Get anomalies\n",
    "anomalies = df[df['anomaly']==1]\n",
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(294588, 14730)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df), len(anomalies)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
